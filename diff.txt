nbdiff Regression.ipynb (HEAD) Regression.ipynb
--- Regression.ipynb (HEAD)  (no timestamp)
+++ Regression.ipynb  2021-04-01 10:23:17.873238
## modified /cells/1/source:
@@ -2,7 +2,7 @@
 !pip install pytest
 # Use seaborn for pairplot
 !pip install seaborn
-#another test
-#testing git diff
+#another test 1
+#testing git diff2
 # Use some functions from tensorflow_docs
 !pip install git+https://github.com/tensorflow/docs


## inserted before /cells/2/outputs/0:
+  output:
+    output_type: error
+    ename: ModuleNotFoundError
+    evalue: No module named 'tensorflow_docs'
+    traceback:
+      item[0]: ---------------------------------------------------------------------------
+      item[1]: ModuleNotFoundError                       Traceback (most recent call last)
+      item[2]:
+        <ipython-input-2-1b8e4cbe56e5> in <module>
+             17 from tensorflow import keras
+             18 from tensorflow.keras import layers
+        ---> 19 import tensorflow_docs as tfdocs
+             20 import tensorflow_docs.plots
+             21 import tensorflow_docs.modeling
+      item[3]: ModuleNotFoundError: No module named 'tensorflow_docs'

## deleted /cells/2/outputs/0-3:
-  output:
-    output_type: stream
-    name: stdout
-    text:
-      2.4.1
-      Downloading data from http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data
-      32768/30286 [================================] - 0s 0us/step
-      Model: "sequential"
-      _________________________________________________________________
-      Layer (type)                 Output Shape              Param #   
-      =================================================================
-      dense (Dense)                (None, 64)                640       
-      _________________________________________________________________
-      dense_1 (Dense)              (None, 64)                4160      
-      _________________________________________________________________
-      dense_2 (Dense)              (None, 1)                 65        
-      =================================================================
-      Total params: 4,865
-      Trainable params: 4,865
-      Non-trainable params: 0
-      _________________________________________________________________
-      
-      Epoch: 0, loss:584.0541,  mae:22.9625,  mse:584.0541,  val_loss:575.3824,  val_mae:22.7874,  val_mse:575.3824,  
-      ....................................................................................................
-      Epoch: 100, loss:6.0367,  mae:1.7258,  mse:6.0367,  val_loss:8.6820,  val_mae:2.1887,  val_mse:8.6820,  
-      ....................................................................................................
-      Epoch: 200, loss:5.0733,  mae:1.5105,  mse:5.0733,  val_loss:7.8719,  val_mae:2.1392,  val_mse:7.8719,  
-      ....................................................................................................
-      Epoch: 300, loss:4.6170,  mae:1.4681,  mse:4.6170,  val_loss:8.0903,  val_mae:2.1609,  val_mse:8.0903,  
-      ....................................................................................................
-      Epoch: 400, loss:4.1383,  mae:1.3470,  mse:4.1383,  val_loss:8.2207,  val_mae:2.2282,  val_mse:8.2207,  
-      ....................................................................................................
-      Epoch: 500, loss:3.8013,  mae:1.3075,  mse:3.8013,  val_loss:8.6059,  val_mae:2.2961,  val_mse:8.6059,  
-      ....................................................................................................
-      Epoch: 600, loss:3.4076,  mae:1.2067,  mse:3.4076,  val_loss:8.1538,  val_mae:2.1818,  val_mse:8.1538,  
-      ....................................................................................................
-      Epoch: 700, loss:3.3700,  mae:1.1993,  mse:3.3700,  val_loss:7.7876,  val_mae:2.1835,  val_mse:7.7876,  
-      ....................................................................................................
-      Epoch: 800, loss:2.8138,  mae:1.0719,  mse:2.8138,  val_loss:8.5772,  val_mae:2.2702,  val_mse:8.5772,  
-      ....................................................................................................
-      Epoch: 900, loss:2.5665,  mae:1.0190,  mse:2.5665,  val_loss:9.7987,  val_mae:2.4192,  val_mse:9.7987,  
-      ....................................................................................................           loss        mae         mse    val_loss    val_mae     val_mse
-      0    584.054138  22.962454  584.054138  575.382446  22.787373  575.382446
-      1    533.281921  21.890244  533.281921  524.036072  21.689587  524.036072
-      2    485.301270  20.837751  485.301270  467.880035  20.419964  467.880035
-      3    432.285675  19.593134  432.285675  407.450623  18.955175  407.450623
-      4    375.510529  18.178080  375.510529  343.600098  17.264168  343.600098
-      ..          ...        ...         ...         ...        ...         ...
-      995    2.449554   1.003951    2.449554    9.177459   2.335821    9.177459
-      996    2.325155   0.976401    2.325155    9.064754   2.340883    9.064754
-      997    2.491637   1.021229    2.491637    7.521409   2.104230    7.521409
-      998    2.320058   0.950841    2.320058    8.003095   2.160827    8.003095
-      999    2.543772   0.975842    2.543772    8.122514   2.161257    8.122514
-      
-      [1000 rows x 6 columns]
-  output:
-    output_type: display_data
-    data:
-      image/png: iVBORw0K...<snip base64, md5=2254965147fdf864...>
-      text/plain: <Figure size 720x720 with 20 Axes>
-    metadata (unknown keys):
-      needs_background: light
-      tags:
-        []
-  output:
-    output_type: stream
-    name: stdout
-    text:
-      
-      Epoch: 0, loss:561.6708,  mae:22.3387,  mse:561.6708,  val_loss:550.2819,  val_mae:21.9660,  val_mse:550.2819,  
-      .........................................................3/3 - 0s - loss: 6.0585 - mae: 1.8495 - mse: 6.0585
-      Testing set Mean Abs Error:  1.85 MPG
-      Testing set MSE:  6.06 MPG
-      Testing set Loss:  6.06 MPG
-  output:
-    output_type: display_data
-    data:
-      image/png: iVBORw0K...<snip base64, md5=1dd09aa8c8652272...>
-      text/plain: <Figure size 432x288 with 1 Axes>
-    metadata (unknown keys):
-      needs_background: light
-      tags:
-        []

## modified /cells/2/source:
@@ -22,9 +22,10 @@ import tensorflow_docs.modeling
 
 print(tf.__version__)
 
-#Downloading the dataset
-dataset_path = keras.utils.get_file("auto-mpg.data", "http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data")
-dataset_path
+#defining function:Downloading the dataset
+def downloading_dataset(name,address):
+    dataset_path = keras.utils.get_file("auto-mpg.data", "http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data")
+return dataset_path
 
 #Importing the data using Pandas library
 column_names = ['MPG','Cylinders','Displacement','Horsepower','Weight', 'Acceleration', 'Model Year', 'Origin']

nbdiff .ipynb_checkpoints/Regression-checkpoint.ipynb (c3344db81911883a705fc6f120fa01a2337418ad) .ipynb_checkpoints/Regression-checkpoint.ipynb
--- .ipynb_checkpoints/Regression-checkpoint.ipynb (c3344db81911883a705fc6f120fa01a2337418ad)  (no timestamp)
+++ .ipynb_checkpoints/Regression-checkpoint.ipynb  2021-04-01 14:10:41.728627
## added /cells/1/metadata/collapsed:
+  True

## modified /cells/1/source:
@@ -2,6 +2,7 @@
 !pip install pytest
 # Use seaborn for pairplot
 !pip install seaborn
-#testing git diff
+#another test 1
+#testing git diff2
 # Use some functions from tensorflow_docs
 !pip install git+https://github.com/tensorflow/docs


## inserted before /cells/2/outputs/0:
+  output:
+    output_type: error
+    ename: ModuleNotFoundError
+    evalue: No module named 'tensorflow_docs'
+    traceback:
+      item[0]: ---------------------------------------------------------------------------
+      item[1]: ModuleNotFoundError                       Traceback (most recent call last)
+      item[2]:
+        <ipython-input-2-1b8e4cbe56e5> in <module>
+             17 from tensorflow import keras
+             18 from tensorflow.keras import layers
+        ---> 19 import tensorflow_docs as tfdocs
+             20 import tensorflow_docs.plots
+             21 import tensorflow_docs.modeling
+      item[3]: ModuleNotFoundError: No module named 'tensorflow_docs'

## deleted /cells/2/outputs/0-3:
-  output:
-    output_type: stream
-    name: stdout
-    text:
-      2.4.1
-      Downloading data from http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data
-      32768/30286 [================================] - 0s 0us/step
-      Model: "sequential"
-      _________________________________________________________________
-      Layer (type)                 Output Shape              Param #   
-      =================================================================
-      dense (Dense)                (None, 64)                640       
-      _________________________________________________________________
-      dense_1 (Dense)              (None, 64)                4160      
-      _________________________________________________________________
-      dense_2 (Dense)              (None, 1)                 65        
-      =================================================================
-      Total params: 4,865
-      Trainable params: 4,865
-      Non-trainable params: 0
-      _________________________________________________________________
-      
-      Epoch: 0, loss:584.0541,  mae:22.9625,  mse:584.0541,  val_loss:575.3824,  val_mae:22.7874,  val_mse:575.3824,  
-      ....................................................................................................
-      Epoch: 100, loss:6.0367,  mae:1.7258,  mse:6.0367,  val_loss:8.6820,  val_mae:2.1887,  val_mse:8.6820,  
-      ....................................................................................................
-      Epoch: 200, loss:5.0733,  mae:1.5105,  mse:5.0733,  val_loss:7.8719,  val_mae:2.1392,  val_mse:7.8719,  
-      ....................................................................................................
-      Epoch: 300, loss:4.6170,  mae:1.4681,  mse:4.6170,  val_loss:8.0903,  val_mae:2.1609,  val_mse:8.0903,  
-      ....................................................................................................
-      Epoch: 400, loss:4.1383,  mae:1.3470,  mse:4.1383,  val_loss:8.2207,  val_mae:2.2282,  val_mse:8.2207,  
-      ....................................................................................................
-      Epoch: 500, loss:3.8013,  mae:1.3075,  mse:3.8013,  val_loss:8.6059,  val_mae:2.2961,  val_mse:8.6059,  
-      ....................................................................................................
-      Epoch: 600, loss:3.4076,  mae:1.2067,  mse:3.4076,  val_loss:8.1538,  val_mae:2.1818,  val_mse:8.1538,  
-      ....................................................................................................
-      Epoch: 700, loss:3.3700,  mae:1.1993,  mse:3.3700,  val_loss:7.7876,  val_mae:2.1835,  val_mse:7.7876,  
-      ....................................................................................................
-      Epoch: 800, loss:2.8138,  mae:1.0719,  mse:2.8138,  val_loss:8.5772,  val_mae:2.2702,  val_mse:8.5772,  
-      ....................................................................................................
-      Epoch: 900, loss:2.5665,  mae:1.0190,  mse:2.5665,  val_loss:9.7987,  val_mae:2.4192,  val_mse:9.7987,  
-      ....................................................................................................           loss        mae         mse    val_loss    val_mae     val_mse
-      0    584.054138  22.962454  584.054138  575.382446  22.787373  575.382446
-      1    533.281921  21.890244  533.281921  524.036072  21.689587  524.036072
-      2    485.301270  20.837751  485.301270  467.880035  20.419964  467.880035
-      3    432.285675  19.593134  432.285675  407.450623  18.955175  407.450623
-      4    375.510529  18.178080  375.510529  343.600098  17.264168  343.600098
-      ..          ...        ...         ...         ...        ...         ...
-      995    2.449554   1.003951    2.449554    9.177459   2.335821    9.177459
-      996    2.325155   0.976401    2.325155    9.064754   2.340883    9.064754
-      997    2.491637   1.021229    2.491637    7.521409   2.104230    7.521409
-      998    2.320058   0.950841    2.320058    8.003095   2.160827    8.003095
-      999    2.543772   0.975842    2.543772    8.122514   2.161257    8.122514
-      
-      [1000 rows x 6 columns]
-  output:
-    output_type: display_data
-    data:
-      image/png: iVBORw0K...<snip base64, md5=2254965147fdf864...>
-      text/plain: <Figure size 720x720 with 20 Axes>
-    metadata (unknown keys):
-      needs_background: light
-      tags:
-        []
-  output:
-    output_type: stream
-    name: stdout
-    text:
-      
-      Epoch: 0, loss:561.6708,  mae:22.3387,  mse:561.6708,  val_loss:550.2819,  val_mae:21.9660,  val_mse:550.2819,  
-      .........................................................3/3 - 0s - loss: 6.0585 - mae: 1.8495 - mse: 6.0585
-      Testing set Mean Abs Error:  1.85 MPG
-      Testing set MSE:  6.06 MPG
-      Testing set Loss:  6.06 MPG
-  output:
-    output_type: display_data
-    data:
-      image/png: iVBORw0K...<snip base64, md5=1dd09aa8c8652272...>
-      text/plain: <Figure size 432x288 with 1 Axes>
-    metadata (unknown keys):
-      needs_background: light
-      tags:
-        []

## modified /cells/2/source:
@@ -21,40 +21,47 @@ import tensorflow_docs.plots
 import tensorflow_docs.modeling
 
 print(tf.__version__)
-
-#Downloading the dataset
-dataset_path = keras.utils.get_file("auto-mpg.data", "http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data")
-dataset_path
-
-#Importing the data using Pandas library
+name_db = "auto-mpg.data"
+address_db = "http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data"
 column_names = ['MPG','Cylinders','Displacement','Horsepower','Weight', 'Acceleration', 'Model Year', 'Origin']
-raw_dataset = pd.read_csv(dataset_path, names=column_names, na_values = "?", comment='\t', sep=" ", skipinitialspace=True)
-dataset = raw_dataset.copy()
-dataset.tail()
-
-#Cleanin the data
-dataset.isna().sum()
-dataset = dataset.dropna()
-dataset['Origin'] = dataset['Origin'].map({1: 'USA', 2: 'Europe', 3: 'Japan'})
-dataset = pd.get_dummies(dataset, prefix='', prefix_sep='')
-dataset.tail()
+EPOCHS = 1000
+#defining function:Downloading the dataset
+def downloading_dataset(name,address):
+    dataset_path = keras.utils.get_file(name, address)
+    return dataset_path
 
+#Importing the data using Pandas library
+def preprocessing_dataset(dataset_path,column_names):
+    raw_dataset = pd.read_csv(dataset_path, names=column_names, na_values = "?", comment='\t', sep=" ", skipinitialspace=True)
+    dataset = raw_dataset.copy()
+    dataset.tail()
+    #Cleanin the data
+    dataset.isna().sum()
+    dataset = dataset.dropna()
+    dataset['Origin'] = dataset['Origin'].map({1: 'USA', 2: 'Europe', 3: 'Japan'})
+    dataset = pd.get_dummies(dataset, prefix='', prefix_sep='')
+    return dataset
 #Spliting the data into train and test
-train_dataset = dataset.sample(frac=0.8,random_state=0)
-test_dataset = dataset.drop(train_dataset.index)
-sns.pairplot(train_dataset[["MPG", "Cylinders", "Displacement", "Weight"]], diag_kind="kde") #Graph
-#overall statistics
-train_stats = train_dataset.describe()
-train_stats.pop("MPG")
-train_stats = train_stats.transpose()
-train_stats
+def split_dataset(dataset):
+    train_dataset = dataset.sample(frac=0.8,random_state=0)
+    test_dataset = dataset.drop(train_dataset.index)
+    sns.pairplot(train_dataset[["MPG", "Cylinders", "Displacement", "Weight"]], diag_kind="kde") #Graph
+    #overall statistics
+    train_stats = train_dataset.describe()
+    train_stats.pop("MPG")
+    train_stats = train_stats.transpose()
+    return train_stats,train_dataset,test_dataset
 
 #Splitting features from labels
-train_labels = train_dataset.pop('MPG')
-test_labels = test_dataset.pop('MPG')
+def split_features(train_dataset,test_dataset):
+    train_labels = train_dataset.pop('MPG')
+    test_labels = test_dataset.pop('MPG')
+    return   train_labels, test_labels
+
 #Normalizing the data
-def norm(x):
-  return (x - train_stats['mean']) / train_stats['std']
+def norm(dataset,train_stats):
+    return (dataset- train_stats['mean']) / train_stats['std']
+
 normed_train_data = norm(train_dataset)
 normed_test_data = norm(test_dataset)
 
@@ -68,59 +75,64 @@ def build_model():
 model = build_model()
 
 #Inspecting the model
-model.summary()
-example_batch = normed_train_data[:10]
-example_result = model.predict(example_batch)
-example_result
+def inspect_model(normed_train_data):
+    model.summary()
+    example_batch = normed_train_data[:10]
+    example_result = model.predict(example_batch)
+    return example_result
 
 #Training the model
-EPOCHS = 1000
 
-history = model.fit(normed_train_data, train_labels, epochs=EPOCHS, validation_split = 0.2, 
+def train_model(model,normed_train_data, train_labels,EP):
+    history = model.fit(normed_train_data, train_labels, epochs=EPOCHS, validation_split = 0.2, 
                     verbose=0, callbacks=[tfdocs.modeling.EpochDots()])
-hist = pd.DataFrame(history.history)
-print(hist, end='\n')
-hist['epoch'] = history.epoch
-hist.tail()
-plotter = tfdocs.plots.HistoryPlotter(smoothing_std=2)
-plotter.plot({'Basic': history}, metric = "mae")
-plt.ylim([0, 10])
-plt.ylabel('MAE [MPG]')
-plotter.plot({'Basic': history}, metric = "mse")
-plt.ylim([0, 20])
-plt.ylabel('MSE [MPG^2]')
-plt.show()
+    hist = pd.DataFrame(history.history)
+    print(hist, end='\n')
+    hist['epoch'] = history.epoch
+    hist.tail()
+    plotter = tfdocs.plots.HistoryPlotter(smoothing_std=2)
+    plotter.plot({'Basic': history}, metric = "mae")
+    plt.ylim([0, 10])
+    plt.ylabel('MAE [MPG]')
+    plotter.plot({'Basic': history}, metric = "mse")
+    plt.ylim([0, 20])
+    plt.ylabel('MSE [MPG^2]')
+    return plotter, plt.show()
 
 model = build_model()
 
-# The patience parameter is the amount of epochs to check for improvement
-early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)
-
-early_history = model.fit(normed_train_data, train_labels, 
+#Making prediction
+def prediction():
+    test_predictions = model.predict(normed_test_data).flatten()
+    a = plt.axes(aspect='equal')
+    plt.scatter(test_labels, test_predictions)
+    plt.xlabel('True Values [MPG]')
+    plt.ylabel('Predictions [MPG]')
+    lims = [0, 50]
+    plt.xlim(lims)
+    plt.ylim(lims)
+    x_ = plt.plot(lims, lims)
+
+    error = test_predictions - test_labels
+    plt.hist(error, bins = 25)
+    plt.xlabel("Prediction Error [MPG]")
+    x_ = plt.ylabel("Count")
+    return plt.show()
+
+def plot_early_stop(plotter):
+    # The patience parameter is the amount of epochs to check for improvement
+    early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)
+
+    early_history = model.fit(normed_train_data, train_labels, 
                     epochs=EPOCHS, validation_split = 0.2, verbose=0, 
                     callbacks=[early_stop, tfdocs.modeling.EpochDots()])
-plotter.plot({'Early Stopping': early_history}, metric = "mae")
-plt.ylim([0, 10])
-plt.ylabel('MAE [MPG]')
-
-loss, mae, mse = model.evaluate(normed_test_data, test_labels, verbose=2)
-print("Testing set Mean Abs Error: {:5.2f} MPG".format(mae))
-print("Testing set MSE: {:5.2f} MPG".format(mse))
-print("Testing set Loss: {:5.2f} MPG".format(loss))
-
-#Making prediction
-test_predictions = model.predict(normed_test_data).flatten()
-a = plt.axes(aspect='equal')
-plt.scatter(test_labels, test_predictions)
-plt.xlabel('True Values [MPG]')
-plt.ylabel('Predictions [MPG]')
-lims = [0, 50]
-plt.xlim(lims)
-plt.ylim(lims)
-x_ = plt.plot(lims, lims)
-
-error = test_predictions - test_labels
-plt.hist(error, bins = 25)
-plt.xlabel("Prediction Error [MPG]")
-x_ = plt.ylabel("Count")
- 

+    plotter.plot({'Early Stopping': early_history}, metric = "mae")
+    plt.ylim([0, 10])
+    plt.ylabel('MAE [MPG]')
+    return plt.show()
+def model_evaluate():
+    loss, mae, mse = model.evaluate(normed_test_data, test_labels, verbose=2)
+    print("Testing set Mean Abs Error: {:5.2f} MPG".format(mae))
+    print("Testing set MSE: {:5.2f} MPG".format(mse))
+    print("Testing set Loss: {:5.2f} MPG".format(loss))
+    return 

nbdiff Regression.ipynb (c3344db81911883a705fc6f120fa01a2337418ad) Regression.ipynb
--- Regression.ipynb (c3344db81911883a705fc6f120fa01a2337418ad)  (no timestamp)
+++ Regression.ipynb  2021-04-01 14:10:41.728627
## added /cells/1/metadata/collapsed:
+  True

## modified /cells/1/source:
@@ -2,6 +2,7 @@
 !pip install pytest
 # Use seaborn for pairplot
 !pip install seaborn
-#testing git diff
+#another test 1
+#testing git diff2
 # Use some functions from tensorflow_docs
 !pip install git+https://github.com/tensorflow/docs


## inserted before /cells/2/outputs/0:
+  output:
+    output_type: error
+    ename: ModuleNotFoundError
+    evalue: No module named 'tensorflow_docs'
+    traceback:
+      item[0]: ---------------------------------------------------------------------------
+      item[1]: ModuleNotFoundError                       Traceback (most recent call last)
+      item[2]:
+        <ipython-input-2-1b8e4cbe56e5> in <module>
+             17 from tensorflow import keras
+             18 from tensorflow.keras import layers
+        ---> 19 import tensorflow_docs as tfdocs
+             20 import tensorflow_docs.plots
+             21 import tensorflow_docs.modeling
+      item[3]: ModuleNotFoundError: No module named 'tensorflow_docs'

## deleted /cells/2/outputs/0-3:
-  output:
-    output_type: stream
-    name: stdout
-    text:
-      2.4.1
-      Downloading data from http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data
-      32768/30286 [================================] - 0s 0us/step
-      Model: "sequential"
-      _________________________________________________________________
-      Layer (type)                 Output Shape              Param #   
-      =================================================================
-      dense (Dense)                (None, 64)                640       
-      _________________________________________________________________
-      dense_1 (Dense)              (None, 64)                4160      
-      _________________________________________________________________
-      dense_2 (Dense)              (None, 1)                 65        
-      =================================================================
-      Total params: 4,865
-      Trainable params: 4,865
-      Non-trainable params: 0
-      _________________________________________________________________
-      
-      Epoch: 0, loss:584.0541,  mae:22.9625,  mse:584.0541,  val_loss:575.3824,  val_mae:22.7874,  val_mse:575.3824,  
-      ....................................................................................................
-      Epoch: 100, loss:6.0367,  mae:1.7258,  mse:6.0367,  val_loss:8.6820,  val_mae:2.1887,  val_mse:8.6820,  
-      ....................................................................................................
-      Epoch: 200, loss:5.0733,  mae:1.5105,  mse:5.0733,  val_loss:7.8719,  val_mae:2.1392,  val_mse:7.8719,  
-      ....................................................................................................
-      Epoch: 300, loss:4.6170,  mae:1.4681,  mse:4.6170,  val_loss:8.0903,  val_mae:2.1609,  val_mse:8.0903,  
-      ....................................................................................................
-      Epoch: 400, loss:4.1383,  mae:1.3470,  mse:4.1383,  val_loss:8.2207,  val_mae:2.2282,  val_mse:8.2207,  
-      ....................................................................................................
-      Epoch: 500, loss:3.8013,  mae:1.3075,  mse:3.8013,  val_loss:8.6059,  val_mae:2.2961,  val_mse:8.6059,  
-      ....................................................................................................
-      Epoch: 600, loss:3.4076,  mae:1.2067,  mse:3.4076,  val_loss:8.1538,  val_mae:2.1818,  val_mse:8.1538,  
-      ....................................................................................................
-      Epoch: 700, loss:3.3700,  mae:1.1993,  mse:3.3700,  val_loss:7.7876,  val_mae:2.1835,  val_mse:7.7876,  
-      ....................................................................................................
-      Epoch: 800, loss:2.8138,  mae:1.0719,  mse:2.8138,  val_loss:8.5772,  val_mae:2.2702,  val_mse:8.5772,  
-      ....................................................................................................
-      Epoch: 900, loss:2.5665,  mae:1.0190,  mse:2.5665,  val_loss:9.7987,  val_mae:2.4192,  val_mse:9.7987,  
-      ....................................................................................................           loss        mae         mse    val_loss    val_mae     val_mse
-      0    584.054138  22.962454  584.054138  575.382446  22.787373  575.382446
-      1    533.281921  21.890244  533.281921  524.036072  21.689587  524.036072
-      2    485.301270  20.837751  485.301270  467.880035  20.419964  467.880035
-      3    432.285675  19.593134  432.285675  407.450623  18.955175  407.450623
-      4    375.510529  18.178080  375.510529  343.600098  17.264168  343.600098
-      ..          ...        ...         ...         ...        ...         ...
-      995    2.449554   1.003951    2.449554    9.177459   2.335821    9.177459
-      996    2.325155   0.976401    2.325155    9.064754   2.340883    9.064754
-      997    2.491637   1.021229    2.491637    7.521409   2.104230    7.521409
-      998    2.320058   0.950841    2.320058    8.003095   2.160827    8.003095
-      999    2.543772   0.975842    2.543772    8.122514   2.161257    8.122514
-      
-      [1000 rows x 6 columns]
-  output:
-    output_type: display_data
-    data:
-      image/png: iVBORw0K...<snip base64, md5=2254965147fdf864...>
-      text/plain: <Figure size 720x720 with 20 Axes>
-    metadata (unknown keys):
-      needs_background: light
-      tags:
-        []
-  output:
-    output_type: stream
-    name: stdout
-    text:
-      
-      Epoch: 0, loss:561.6708,  mae:22.3387,  mse:561.6708,  val_loss:550.2819,  val_mae:21.9660,  val_mse:550.2819,  
-      .........................................................3/3 - 0s - loss: 6.0585 - mae: 1.8495 - mse: 6.0585
-      Testing set Mean Abs Error:  1.85 MPG
-      Testing set MSE:  6.06 MPG
-      Testing set Loss:  6.06 MPG
-  output:
-    output_type: display_data
-    data:
-      image/png: iVBORw0K...<snip base64, md5=1dd09aa8c8652272...>
-      text/plain: <Figure size 432x288 with 1 Axes>
-    metadata (unknown keys):
-      needs_background: light
-      tags:
-        []

## modified /cells/2/source:
@@ -21,40 +21,47 @@ import tensorflow_docs.plots
 import tensorflow_docs.modeling
 
 print(tf.__version__)
-
-#Downloading the dataset
-dataset_path = keras.utils.get_file("auto-mpg.data", "http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data")
-dataset_path
-
-#Importing the data using Pandas library
+name_db = "auto-mpg.data"
+address_db = "http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data"
 column_names = ['MPG','Cylinders','Displacement','Horsepower','Weight', 'Acceleration', 'Model Year', 'Origin']
-raw_dataset = pd.read_csv(dataset_path, names=column_names, na_values = "?", comment='\t', sep=" ", skipinitialspace=True)
-dataset = raw_dataset.copy()
-dataset.tail()
-
-#Cleanin the data
-dataset.isna().sum()
-dataset = dataset.dropna()
-dataset['Origin'] = dataset['Origin'].map({1: 'USA', 2: 'Europe', 3: 'Japan'})
-dataset = pd.get_dummies(dataset, prefix='', prefix_sep='')
-dataset.tail()
+EPOCHS = 1000
+#defining function:Downloading the dataset
+def downloading_dataset(name,address):
+    dataset_path = keras.utils.get_file(name, address)
+    return dataset_path
 
+#Importing the data using Pandas library
+def preprocessing_dataset(dataset_path,column_names):
+    raw_dataset = pd.read_csv(dataset_path, names=column_names, na_values = "?", comment='\t', sep=" ", skipinitialspace=True)
+    dataset = raw_dataset.copy()
+    dataset.tail()
+    #Cleanin the data
+    dataset.isna().sum()
+    dataset = dataset.dropna()
+    dataset['Origin'] = dataset['Origin'].map({1: 'USA', 2: 'Europe', 3: 'Japan'})
+    dataset = pd.get_dummies(dataset, prefix='', prefix_sep='')
+    return dataset
 #Spliting the data into train and test
-train_dataset = dataset.sample(frac=0.8,random_state=0)
-test_dataset = dataset.drop(train_dataset.index)
-sns.pairplot(train_dataset[["MPG", "Cylinders", "Displacement", "Weight"]], diag_kind="kde") #Graph
-#overall statistics
-train_stats = train_dataset.describe()
-train_stats.pop("MPG")
-train_stats = train_stats.transpose()
-train_stats
+def split_dataset(dataset):
+    train_dataset = dataset.sample(frac=0.8,random_state=0)
+    test_dataset = dataset.drop(train_dataset.index)
+    sns.pairplot(train_dataset[["MPG", "Cylinders", "Displacement", "Weight"]], diag_kind="kde") #Graph
+    #overall statistics
+    train_stats = train_dataset.describe()
+    train_stats.pop("MPG")
+    train_stats = train_stats.transpose()
+    return train_stats,train_dataset,test_dataset
 
 #Splitting features from labels
-train_labels = train_dataset.pop('MPG')
-test_labels = test_dataset.pop('MPG')
+def split_features(train_dataset,test_dataset):
+    train_labels = train_dataset.pop('MPG')
+    test_labels = test_dataset.pop('MPG')
+    return   train_labels, test_labels
+
 #Normalizing the data
-def norm(x):
-  return (x - train_stats['mean']) / train_stats['std']
+def norm(dataset,train_stats):
+    return (dataset- train_stats['mean']) / train_stats['std']
+
 normed_train_data = norm(train_dataset)
 normed_test_data = norm(test_dataset)
 
@@ -68,59 +75,64 @@ def build_model():
 model = build_model()
 
 #Inspecting the model
-model.summary()
-example_batch = normed_train_data[:10]
-example_result = model.predict(example_batch)
-example_result
+def inspect_model(normed_train_data):
+    model.summary()
+    example_batch = normed_train_data[:10]
+    example_result = model.predict(example_batch)
+    return example_result
 
 #Training the model
-EPOCHS = 1000
 
-history = model.fit(normed_train_data, train_labels, epochs=EPOCHS, validation_split = 0.2, 
+def train_model(model,normed_train_data, train_labels,EP):
+    history = model.fit(normed_train_data, train_labels, epochs=EPOCHS, validation_split = 0.2, 
                     verbose=0, callbacks=[tfdocs.modeling.EpochDots()])
-hist = pd.DataFrame(history.history)
-print(hist, end='\n')
-hist['epoch'] = history.epoch
-hist.tail()
-plotter = tfdocs.plots.HistoryPlotter(smoothing_std=2)
-plotter.plot({'Basic': history}, metric = "mae")
-plt.ylim([0, 10])
-plt.ylabel('MAE [MPG]')
-plotter.plot({'Basic': history}, metric = "mse")
-plt.ylim([0, 20])
-plt.ylabel('MSE [MPG^2]')
-plt.show()
+    hist = pd.DataFrame(history.history)
+    print(hist, end='\n')
+    hist['epoch'] = history.epoch
+    hist.tail()
+    plotter = tfdocs.plots.HistoryPlotter(smoothing_std=2)
+    plotter.plot({'Basic': history}, metric = "mae")
+    plt.ylim([0, 10])
+    plt.ylabel('MAE [MPG]')
+    plotter.plot({'Basic': history}, metric = "mse")
+    plt.ylim([0, 20])
+    plt.ylabel('MSE [MPG^2]')
+    return plotter, plt.show()
 
 model = build_model()
 
-# The patience parameter is the amount of epochs to check for improvement
-early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)
-
-early_history = model.fit(normed_train_data, train_labels, 
+#Making prediction
+def prediction():
+    test_predictions = model.predict(normed_test_data).flatten()
+    a = plt.axes(aspect='equal')
+    plt.scatter(test_labels, test_predictions)
+    plt.xlabel('True Values [MPG]')
+    plt.ylabel('Predictions [MPG]')
+    lims = [0, 50]
+    plt.xlim(lims)
+    plt.ylim(lims)
+    x_ = plt.plot(lims, lims)
+
+    error = test_predictions - test_labels
+    plt.hist(error, bins = 25)
+    plt.xlabel("Prediction Error [MPG]")
+    x_ = plt.ylabel("Count")
+    return plt.show()
+
+def plot_early_stop(plotter):
+    # The patience parameter is the amount of epochs to check for improvement
+    early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)
+
+    early_history = model.fit(normed_train_data, train_labels, 
                     epochs=EPOCHS, validation_split = 0.2, verbose=0, 
                     callbacks=[early_stop, tfdocs.modeling.EpochDots()])
-plotter.plot({'Early Stopping': early_history}, metric = "mae")
-plt.ylim([0, 10])
-plt.ylabel('MAE [MPG]')
-
-loss, mae, mse = model.evaluate(normed_test_data, test_labels, verbose=2)
-print("Testing set Mean Abs Error: {:5.2f} MPG".format(mae))
-print("Testing set MSE: {:5.2f} MPG".format(mse))
-print("Testing set Loss: {:5.2f} MPG".format(loss))
-
-#Making prediction
-test_predictions = model.predict(normed_test_data).flatten()
-a = plt.axes(aspect='equal')
-plt.scatter(test_labels, test_predictions)
-plt.xlabel('True Values [MPG]')
-plt.ylabel('Predictions [MPG]')
-lims = [0, 50]
-plt.xlim(lims)
-plt.ylim(lims)
-x_ = plt.plot(lims, lims)
-
-error = test_predictions - test_labels
-plt.hist(error, bins = 25)
-plt.xlabel("Prediction Error [MPG]")
-x_ = plt.ylabel("Count")
- 

+    plotter.plot({'Early Stopping': early_history}, metric = "mae")
+    plt.ylim([0, 10])
+    plt.ylabel('MAE [MPG]')
+    return plt.show()
+def model_evaluate():
+    loss, mae, mse = model.evaluate(normed_test_data, test_labels, verbose=2)
+    print("Testing set Mean Abs Error: {:5.2f} MPG".format(mae))
+    print("Testing set MSE: {:5.2f} MPG".format(mse))
+    print("Testing set Loss: {:5.2f} MPG".format(loss))
+    return 

